# 播放器一般原理

[TOC]



播放媒体文件的基本模块，

读文件模块

解复用模块

视频解码模块

音频解码模块

颜色空间转换模块

视频显示模块

音频播放模块



按filter可以粗略分为五类：

##Source filer, 为Demux filter提供数据流，可从本地文件或者网络获取，并屏蔽二者的差别

读文件模块，可以简单分为3层，最底层的是file,pipe,tcp,udp,http等这些具体的本地文件或网络协议；

中间抽象层用URLContext结构来统一表示底层具体的本地文件或网络协议，相关操作也只是简单的中转一下调用底层具体文件或协议的支撑函数；

最上层用ByteIOContext结构来扩展URLProtocol结构成内部有缓冲机制的广泛意义上的文件，并且仅仅由ByteIOContext对模块外提供服务。此模块主要有 libavformat 目录下的 file.c,avio.h, avio.c, aviobuf.c 等文件，实现读媒体文件功能。



## Demux flter, 解复用过滤器的作用是识别文件类型，媒体类型，分离出各媒体原始数据流，打上时钟信息后送给下级decoder filter。确定类型的方法是通过遍历的方法。

分为两层，底层是AVIContext,TCPContext,UDPCOntext等等这些具体媒体的解复用结构和相关的基础程序。

上层是AVInputFormat结构和相关的程序。

 上下层之间由AVInputFormat
相对应的 AVFormatContext结构的 priv_data 字段关联 AVIContext或 TCPContext 或 UDPContext 等等具体的文件
格式。AVInputFormat和具体的音视频编码算法格式由 AVFormatContext结构的 streams 字段关联媒体格式，streams
相当于 demux filter 的 output pin，解复用模块分离音视频裸数据通过 streams 传递给下级音视频解码器。此模块
主要有 libavformat 目录下的 avidec.c，utils_format.c 文件。



## Decoder filter, 解码数据包，并且把同步时钟信息传递下去。对视频媒体而言，通常是解码成YUV数据。音频，通常解码成PCM数据

也可以简单的分为两层，由AVCodec统一表述，上层是AVCodec对应的AVCodecContext结构和相关的基础程序。底层是TSContext,MsrleContext等等这些具体的编解码内部使用的结构和相关的基础程序。

AVCodecContext结构的 priv_data 字段关联 TSContext，MsrleContext 等等具体解码器上
下文。此模块主要有 libavcodec 目录下的 msrle.c，truespeech.c，truespeech_data.h，utils_codec.c 等文件。



## Color Space converter filter，把视频解码器解码出来的数据转换成当前显示系统支持的颜色格式。

Color space converter 颜色空间转换模块，大多数的 decoder filter 解码出来的数据是 YUV 颜色空间的数据，
并不是所有系统都支持的 YUV 颜色空间。如果不匹配，就需要做颜色空间转换，但是目前 PC 上无论是独立显
卡还是集成显卡都直接支持 YUV 颜色空间显示。有些视频解码器或视频渲染模块含有颜色空间转换功能，这时
就不需要独立的颜色空间转换模块。在本例中，颜色空间转换模块转换RGB8格式到 RGB32格式直接显示。此
模块主要有 libavcodec 目录下的 imgconvert.c，imgconvert_template.h 文件。



## Render filter，渲染过滤器，在适当的时间渲染相应的媒体。

ffplay 中的渲染模块使用 SDL 库，由 SDL 库帮我们显示视频，播放音频。我们只需
要会用 SDL 库中的函数就可以了， 具体的渲染动作可以不用理; 如果确实想SD看L 库是怎样运作的， 那就把SDL
库的源代码找出来，潜心研读，关于这部分实现的讨论不在本书范围内。





结构体分析

```C++
typedef struct AVCodec
{
  const char* name;//标示Codec的名字，
  enum CodecType type;//Codec的类型，有Video,Audio,Data
  enum CodecID id;//
  int priv_data_size;//context大小
  
  int(*init)(AVCodecContext*);//对外提供的操作
  int(*encode)(AVCodecContext *,uint8_t *buf,int buf_size,void *data);
  int(*close)(AVCodecCOntext*);
  int(*decode)(AVCodecCOntext*,void *outdata,int *outdata_size,uint8_t *buf,int buf_size);
  int capabilities;
  struct AVCodec* next;
  
}AVCodec;


typedef struct AVCodecContext
{
  int bit_rate;
  int frame_number;
  
  unsigned char *extradata;
  int extradata_size;
  
  int width,height;//针对视频
  enum PixelFormat pix_fmt;
  
  int sample_rate;//针对音频
  int channels;
  int bits_per_sample;
  int block_align;
  
  struct AVCodec *codec;
  void *priv_data;
  
  enum CodecType codec_type;
  enum CodecID codec_id;
  
  int(*get_buffer)(struct AVCodecContext *c,AVFrame *pic);
  void(*release_buffer)(struct AVCodecContext *c,AVFrame *pic);
  int(*reget_buffer)(struct AVCodecContext *c,AVFrame *pic);
  
  int internal_buffer_count;
  void *internal_buffer;
  
  struct AVPaletteControl *palctrl;
  
}AVCodecContext;
```



AVInputFormat/AVFormatContext/AVIContext

```c++
typedef struct AVInputFormat
{
  const char *name;
  int priv_data_size;//标示具体的文件容器格式对应的context的大小，本例中是AVIcontext
  
  int(*read_probe)(AVProbeData*);
  int(*read_header)(struct AVFormatContext*,AVFormatParameters *ap);
  int(*read_packet)(struct AVFormatContext*,AVPacket *pkt);
  int(*read_close)(struct AVFormatContext*);
  
  const char* extensions;
  struct AVInputFormat *next;
}AVInputFormat;


typedef struct AVFormatContext
{
  struct AVInputFormat *iformat;
  void *priv_data;//指向具体的文件容器格式的上下文Context，在本例中是AVIContext
  
  ByteIOContext pb;//广泛意义的输入文件
  int nb_streams;
  
  AVStream *streams[MAX_STREAMS];
}AVFormatContext;

AVFormatContext结构表示程序运行的当前文件容器格式使用的上下文，着重于所有文件容器共有的属性和关联其他结构的字段。iformat字段关联相应的文件容器格式，pb 关联广义的 输入文件;streams 关联音视频流;priv_data 字段关联各个具体文件容器独有的属性上下文，和 priv_data_siz e配 对使用。


typedef struct AVIContext
{
  int64_t riff_end;
  int64_t movi_end;
  offset_t movi_list;
  int non_interleaved;
  int stream_index_2;
}AVIContext;

AVIContext定义了AVI中流的一些属性。
```



接着我们来重点分析 URLProtocol/URLContext(ByteIOContext)/FILE(Socket)这几个数据结构，这几个数据结
构定义了读取文件的核心架构，相当于 Directshow 中的文件源 file source filter。

```c++
typedef struct URLProtocol
{
  const char *name;
  int(*url_open)(URLContext *h,const char *filename,int flags);
  int(*url_read)(URLContext *h,unsigned char *buf,int size);
  int(*url_write)(URLContext *h,unsigned char *buf,int size);
  offset_t(*url_seek)(URLContext *h,offset_t pos,int whence);
  int(*url_close)(URLContext *h);
  struct URLProtocol *next;
}URLProtocol;


typedef struct URLContext
{
  struct URLProtocol *prot;
  int flags;
  int max_packet_size;
  void *priv_data;//文件句柄fd，网络通信socket等。
  char filename[1];
}URLContext;

typedef struct ByteIOContext
{
  unsigned char *buffer;
  int buffer_size;
  unsigned char *buf_ptr,*buf_end;
  void *opaque;
  int(*read_buf)(void *opaque,uint8_t *buf,int buf_size);
  int(*write_buf)(void *opaque,uint8_t *buf,int buf_size);
  offset_t(*seek)(void *opaque,offset_t offset,int whence);
  offset_t pos;
  int must_flush;
  int eof_reached;
  int write_flag;
  int max_packet_size;
  int error;
}ByteIOConetxt;

ByteIOContext 结构扩展 URLProtocol 结构成内部有缓冲机制的广泛意义上的文件，改善广义输入文件的 IO
性能。由其数据结构定义的字段可知，主要是缓冲区相关字段，标记字段，和一个关联字段 opaque 来完成广义 文件读写操作。opaque 关联字段用于关联 URLContext 结构，间接关联并扩展 URLProtocol 结构
```



接着我们来重点分析 AVStream/AVIStream这几个数据结构， 这几个数据结构定义了解析媒体流的核心属性，
主要用于读取媒体流数据，相当于 Directshow 中的解复用 Demux 内部的流解析逻辑。特别注意此结构关联
AVCodecContext 结构，并经此结构能跳转到其他结构。



```c++
typedef struct AVStream//解析文件容器内部使用的逻辑
{
  AVCodecContext *actx;
  void *priv_data;//AVIStream
  
  AVRational time_base;
  
  AVIndexEntry *index_entries;
  int nb_index_entries;
  int index_entries_allocated_size;
  
  double frame_last_delay;
}AVStream;

AVStream结构表示当前媒体流的上下文， 着重于所有媒体流共有的属(性并且是在程序运行时才能确定其值)
和关联其他结构的字段。actx 字段关联当前音视频媒体使用的编解码器;priv_data 字段关联解析各个具体媒体流
与文件容器有关的独有的属性;还有一些媒体帧索引和时钟信息。


typedef struct AVIStream
{
  int64_t frame_offset;
  int remaining;
  int packet_size;
  
  int scale;
  int rate;
  int sample_size;
  
  int64_t cum_len;
  
  int prefix;
  int prefix_count;
}AVIStream;

AVIStream 结构定义了 AVI文件中媒体流的一些属性，用于解析 AVI文件。
```



接着我们来分析 AVPacket/AVPacketList/PacketQueue这几个数据结构，这几个数据结构定义解复用demux
模块输出的音视频压缩数据流队列，相当于 Directshow 中 Demux 的 OutputPin，传递数据到解码器。

```c++
typedef struct AVPacket
{
  int64_t pts;
  int64_t dts;
  int64_t pos;
  uint8_t *data;
  int size;
  int stream_index;
  int flags;
  void(*destruct)(struct AVPacket*);
}AVPacket;

AVPacket 代表音视频数据帧，固有的属性是一些标记，时钟信息，和压缩数据首地址，大小等信息。


```









```c++
typedef struct VideoState {
SDL_Thread *parse_tid; SDL_Thread *video_tid;
int abort_request;
AVInputFormat*iformat; AVFormatContext*ic;// 关联的主要数据结构是 ByteIOContext 和 AVStream
  
 AVStream*audio_st; // 关联的主要数据结构是 AVCodecContext 和 AVIStream AVStream*video_st;
int audio_stream; // 音频流索引，实际表示 AVFormatContext结构中 AVStream*streams[]数组中的索引
int video_stream; // 视频流索引，实际表示 AVFormatContext结构中 AVStream*streams[]数组中的索引
PacketQueue audioq; // 音频数据包队列，注意一包音频数据可能包含几个音频帧 PacketQueue videoq; // 视频数据包队列，注意瘦身后的 ffplay 一包视频数据是完整的一帧
VideoPicturepictq[VIDEO_PICTURE_QUEUE_SIZE]; // 输出视频队列，瘦身后的 ffplay 只有一项 double 
frame_last_delay;
uint8_t audio_buf[(AVCODEC_MAX_AUDIO_FRAME_SIZE*3)/2];// 输出的音频缓存
unsigned int audio_buf_siz e;
int audio_buf_index;
AVPacketaudio_pkt; // 音频包属性，只一个指针指向原始音频包数据，非直接包含音频数据包数据 
uint8_t *audio_pkt_data;
int audio_pkt_siz e;
SDL_mutex *video_decoder_mutex; // 视频线程同步互斥变量
SDL_mutex *audio_decoder_mutex; // 音频线程同步互斥变量 char filename[240]; // 媒体文件名
} VideoState;
```



## 简单流程分析

由ByteIOContext（URLContext/URLProtocol）表示的广义输入文件，在AVStream（AVIStream）提供的特定文件容器流信息的指引下，用AVIputFormat（AVFormatContext/AVInputFormat）接口的read_packet（）函数读取完整的一帧数据，分别放到音频或视频PacketQueue（AVPacketList/AVPacket）队列中，这部分功能由decode_thread线程完成。对于视频数据，video_thread线程不停地从视频PacketQueue队列中取出视频帧，调用AVCodecc(AVCodecContex/tMsrleContext)接口的decode（）函数解码视频帧，在适当延时后做颜色空间转化并调用SDL库显示出来，对于音频数据，SDL播放库播放完缓冲区的PCM数据后，调用sdl_audio_callback（）函数解码音频数据，并把解码后的PCM数据填充到SDL音频缓存播放。当下次播放完后，
再调用 sdl_audio_callback()函数解码填充，如此循环不已。对于 SDL 库的实现，这里不做讨论。



特别注意 decode_thread 线程的共分为三大逻辑功能。 

(1): 首先调用av_open_input_file()直接识别文件格式和间接识别媒体格式(媒体格式是通过 av_open_input_file 调用 av_open_input_stream 再调用 ic->iformat->read_header(ic, ap)来识别的，所以是间接识别媒体格式)。 

(2):接着调用 stream_component_open()查找打开编解码器 codec 并启动音频和视频解码线程。
(3):再接着解析文件，分离音视频数据包，排序进入队列。



























